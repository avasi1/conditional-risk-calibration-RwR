{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b75c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from uci_datasets import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from constants import (\n",
    "    ESTIMATED_ERROR_COL,\n",
    "    PREDICTIONS_COL,\n",
    "    SEED,\n",
    "    TARGET_COL,\n",
    "    TRUE_ERROR_COL,\n",
    ")\n",
    "from evaluate_models import create_df_with_results, evaluate_one_instance, round_value_in_table\n",
    "from processing import get_model_f_output_and_compute_losses, get_trained_model_f_and_processed_data\n",
    "from visualisation_functions import plot_conditional_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debda09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for different models\n",
    "random_forest_params = {\n",
    "    'criterion': 'squared_error',\n",
    "    'max_depth': None,\n",
    "    'random_state': SEED,\n",
    "}\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': (64,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'batch_size': 256,\n",
    "    'learning_rate_init': 0.0005,\n",
    "    'max_iter': 800,\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "mlp_params2 = {\n",
    "    'hidden_layer_sizes': (64, 64),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'batch_size': 256,\n",
    "    'learning_rate_init': 0.0005,\n",
    "    'max_iter': 800,\n",
    "    'random_state': SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a02d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and parameters for the evaluation\n",
    "cost_list = [0.2, 0.5, 1, 2]\n",
    "metrics_list = ['RwR_loss', 'human_rate', 'mae', 'rmse', 'bias', 'wape']\n",
    "data_names = ['concrete', 'wine', 'airfoil', 'energy', 'housing', 'solar', 'forest', 'parkinsons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model pairs for comparison (each pair is [model_f, model_L])\n",
    "model_pairs = {\n",
    "    'LR+LR': [Pipeline([('scaler', StandardScaler()), ('lin_reg', LinearRegression())]), Pipeline([('scaler', StandardScaler()), ('lin_reg', LinearRegression())])],\n",
    "    'LR+RF': [Pipeline([('scaler', StandardScaler()), ('lin_reg', LinearRegression())]), RandomForestRegressor(**random_forest_params)],\n",
    "    'LR+MLP': [Pipeline([('scaler', StandardScaler()), ('lin_reg', LinearRegression())]), Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))])],\n",
    "    'LR+MLP2': [Pipeline([('scaler', StandardScaler()), ('lin_reg', LinearRegression())]), Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params2))])],\n",
    "    'RF+LR': [RandomForestRegressor(**random_forest_params), Pipeline([('scaler', StandardScaler()), ('lin_reg', LinearRegression())])],\n",
    "    'RF+RF': [RandomForestRegressor(**random_forest_params), RandomForestRegressor(**random_forest_params)],\n",
    "    'RF+MLP': [RandomForestRegressor(**random_forest_params), Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))])],\n",
    "    'RF+MLP2': [RandomForestRegressor(**random_forest_params), Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params2))])],\n",
    "    'MLP+LR': [Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))]), Pipeline([('scaler', StandardScaler()), ('lin_reg', LinearRegression())])],\n",
    "    'MLP+RF': [Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))]), RandomForestRegressor(**random_forest_params)],\n",
    "    'MLP+MLP': [Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))]), Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))])],\n",
    "    'MLP+MLP2': [Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))]), Pipeline([('scaler', StandardScaler()), ('mlp2', MLPRegressor(**mlp_params2))])],\n",
    "    'MLP2+LR': [Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params2))]), Pipeline([('scaler', StandardScaler()), ('lin_reg', LinearRegression())])],\n",
    "    'MLP2+RF': [Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params2))]), RandomForestRegressor(**random_forest_params)],\n",
    "    'MLP2+MLP': [Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params2))]), Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))])],\n",
    "    'MLP2+MLP2': [Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params2))]), Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params2))])],\n",
    "}\n",
    "\n",
    "# Iterate over each dataset name in data_names\n",
    "for data_name in data_names:\n",
    "    results_all_pairs = dict()\n",
    "\n",
    "    # Iterate over each model pair in model_pairs\n",
    "    for model_name, (model_f, model_L) in model_pairs.items():\n",
    "        results_one_pair = dict()\n",
    "\n",
    "        # Iterate over each cost value in cost_list\n",
    "        for metrics in metrics_list:\n",
    "            results_one_pair[metrics] = []\n",
    "\n",
    "        for cost in cost_list:\n",
    "            print(f\"==== Dataset: {data_name}, model_name: {model_name}, cost: {cost}\")\n",
    "            \n",
    "            data = Dataset(data_name)\n",
    "            results_one_cost = evaluate_one_instance(data, model_f, model_L, cost)\n",
    "            for metrics in metrics_list:\n",
    "                results_one_pair[metrics].append(results_one_cost[metrics])\n",
    "\n",
    "        results_all_pairs[model_name] = results_one_pair\n",
    "\n",
    "    # Create a DataFrame from the results and save to csv files\n",
    "    df_output = create_df_with_results(results_all_pairs, cost_list)\n",
    "    for metrics in metrics_list:\n",
    "        df_output[metrics].to_csv(f'output_folder/{metrics}_{data_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb1cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results for each metric across all datasets\n",
    "for metrics in metrics_list:\n",
    "    df_one_metrics_all_datasets_list = []\n",
    "\n",
    "    # Read and process each dataset's csv file for the current metric\n",
    "    for data_name in data_names:\n",
    "        df_temp = pd.read_csv(f'output_folder/{metrics}_{data_name}.csv')\n",
    "        df_temp = df_temp.reindex(sorted(df_temp.columns), axis=1)\n",
    "        df_temp.insert(loc=0, column='Dataset', value=data_name)\n",
    "        df_one_metrics_all_datasets_list.append(df_temp)\n",
    "\n",
    "    df_one_metrics_all_datasets = pd.concat(df_one_metrics_all_datasets_list, ignore_index=True)\n",
    "\n",
    "    # Round the values in the DataFrame\n",
    "    for column in df_one_metrics_all_datasets.columns[2:]:\n",
    "        df_one_metrics_all_datasets[column] = df_one_metrics_all_datasets[column].apply(round_value_in_table)\n",
    "\n",
    "    # Drop the 'Cost' column and remove duplicates for specific metrics\n",
    "    if metrics not in ['RwR_loss', 'human_rate']:\n",
    "        df_one_metrics_all_datasets = df_one_metrics_all_datasets.drop('Cost', axis=1)\n",
    "        df_one_metrics_all_datasets.drop_duplicates(inplace=True)\n",
    "\n",
    "    df_one_metrics_all_datasets.to_csv(f'output_folder/{metrics}_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d62da0",
   "metadata": {},
   "source": [
    "# 1. RwR Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec620fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.read_csv(f'output_folder/RwR_loss_all.csv', index_col='Unnamed: 0')\n",
    "df_output[list(model_pairs.keys())] = df_output[list(model_pairs.keys())].apply(lambda col: col.str.replace(r'\\s*\\(.*?\\)', '', regex=True))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5ca7e",
   "metadata": {},
   "source": [
    "# 2. Human Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.read_csv(f'output_folder/human_rate_all.csv', index_col='Unnamed: 0')\n",
    "df_output[list(model_pairs.keys())] = df_output[list(model_pairs.keys())].apply(lambda col: col.str.replace(r'\\s*\\(.*?\\)', '', regex=True))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e1c86",
   "metadata": {},
   "source": [
    "# 3. MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7bccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.read_csv(f'output_folder/mae_all.csv', index_col='Unnamed: 0').reset_index(drop=True)\n",
    "df_output[list(model_pairs.keys())] = df_output[list(model_pairs.keys())].apply(lambda col: col.str.replace(r'\\s*\\(.*?\\)', '', regex=True))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd98c9a",
   "metadata": {},
   "source": [
    "# 4. RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af405883",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.read_csv(f'output_folder/rmse_all.csv', index_col='Unnamed: 0').reset_index(drop=True)\n",
    "df_output[list(model_pairs.keys())] = df_output[list(model_pairs.keys())].apply(lambda col: col.str.replace(r'\\s*\\(.*?\\)', '', regex=True))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955adb09",
   "metadata": {},
   "source": [
    "# 5. Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.read_csv(f'output_folder/bias_all.csv', index_col='Unnamed: 0')\n",
    "df_output[list(model_pairs.keys())] = df_output[list(model_pairs.keys())].apply(lambda col: col.str.replace(r'\\s*\\(.*?\\)', '', regex=True))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d18ad",
   "metadata": {},
   "source": [
    "# 6. WAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.read_csv(f'output_folder/wape_all.csv', index_col='Unnamed: 0').reset_index(drop=True)\n",
    "df_output[list(model_pairs.keys())] = df_output[list(model_pairs.keys())].apply(lambda col: col.str.replace(r'\\s*\\(.*?\\)', '', regex=True))\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422ff36",
   "metadata": {},
   "source": [
    "# 7. Visualization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086abd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'energy'\n",
    "data = Dataset(data_name)\n",
    "model_f = Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(**mlp_params))])\n",
    "model_L = RandomForestRegressor(**random_forest_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a945e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model 'f' and prepare datasets for training/testing model 'L'\n",
    "model_f_trained, X_train, X_test, Y_train, Y_test = get_trained_model_f_and_processed_data(\n",
    "    data=data,\n",
    "    split_index=0,\n",
    "    model_f=model_f,\n",
    ")\n",
    "\n",
    "# Obtain predictions from the trained model 'f' and compute errors to train/test model 'L'\n",
    "output_pred_train, output_pred_test, target_train, target_test = get_model_f_output_and_compute_losses(\n",
    "    model_f_trained=model_f_trained,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    Y_train=Y_train,\n",
    "    Y_test=Y_test\n",
    ")\n",
    "\n",
    "# Train model 'L' and obtain estimates of the errors for the test data\n",
    "model_L_trained = model_L.fit(X_train, target_train)\n",
    "target_pred_test = model_L_trained.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19e0411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the actual targets, model 'f' predictions, and losses estimated by model 'L', and sort the dataframe by the target column\n",
    "predictions_and_errors_test = pd.DataFrame(\n",
    "    {TARGET_COL: Y_test,\n",
    "     PREDICTIONS_COL: output_pred_test,\n",
    "     ESTIMATED_ERROR_COL: np.sqrt(target_pred_test), \n",
    "     TRUE_ERROR_COL: target_test}).sort_values(by=TARGET_COL).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82015756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the target values, predictions, and estimated errors\n",
    "plot_conditional_loss(predictions_and_errors_test, f'Dataset: {data_name[0].upper() + data_name[1:]}, Regressor: MLP, Calibrator: RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cefc16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
